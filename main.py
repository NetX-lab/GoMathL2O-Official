import os
import numpy as np
import configargparse
from timeit import default_timer as timer

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

import random
import utils
import optimizers

import time

from optimizees import OPTIMIZEE_DICT

from torch.utils.tensorboard import SummaryWriter

# Argument Parsing
parser = configargparse.get_arg_parser(
    description='Configurations for L2O experiements')

parser.add('-c', '--config', is_config_file=True, help='Config file path.')

# Optimizer options
parser.add('--optimizer', type=str, metavar='STR',
           help='What optimizer to use for the current experiment.')
parser.add('--grad-method', type=str, default='subgrad', metavar='STR',
           help='How to calculate gradients with respect to the objective func.')
parser.add('--cpu', action='store_true',
           help='Force to use CPU instead of GPU even if CUDA compatible GPU '
                'devices are available.')
parser.add('--device', type=str, default=None, help='cuda:0')
parser.add('--test', action='store_true', help='Run in test mode.')
parser.add('--state-scale', type=float, default=0.01, metavar='FLOAT',
           help='scale of the lstm states.')

# Optimizee general options
parser.add('--optimizee-type',
           choices=['QuadraticUnconstrained', 'LASSO',
                    'LogisticL1', 'LogisticL1CIFAR10'],
           help='Type of optimizees to be trained on.')
parser.add('--input-dim', type=int, metavar='INT',
           help='Dimension of the input (optimization variable).')
parser.add('--output-dim', type=int, metavar='INT',
           help='Dimension of the output (labels used to calculate loss).')
parser.add('--rho', type=float, default=0.1, metavar='FLOAT',
           help='Parameter for reg. term in the objective function.')
parser.add('--fixed-dict', action='store_true',
           help='Use a fixed dictionary for the optimizees')
parser.add('--sparsity', type=int, default=5, metavar='INT',
           help='Sparisty of the input variable.')
parser.add('--save-to-mat', action='store_true',
           help='save optmizees to mat file.')
parser.add('--optimizee-dir', type=str, metavar='STR',
           help='dir of optimizees.')
parser.add('--load-mat', action='store_true',
           help='load optmizees from mat file.')
parser.add('--save-sol', action='store_true',
           help='save solutions of optimizees.')
parser.add('--load-sol', action='store_true',
           help='save solutions of optimizees.')
parser.add('--ood', action='store_true',
           help='if use ood problem in optimizee')
parser.add('--ood-s', type=float, default=0.0, metavar='FLOAT',
           help='OOD on s')
parser.add('--ood-t', type=float, default=0.0, metavar='FLOAT',
           help='OOD on t')

# Model parameters
parser.add('--lstm-layers', type=int, default=2, metavar='INT',
           help='Number of layers of the neural network.')
parser.add('--lstm-hidden-size', type=int, default=256, metavar='INT',
           help='Number of layers of the neural network.')

parser.add('--rnnprop-beta1', type=float, default=0.95, metavar='FLOAT',
           help='Adam hyperparameter for RNNprop.')
parser.add('--rnnprop-beta2', type=float, default=0.95, metavar='FLOAT',
           help='Adam hyperparameter for RNNprop.')

parser.add('--r-use', action='store_true',
           help='Use the pre-conditioners generated by LSTM.')
parser.add('--r-scale', type=float, default=1.0, metavar='FLOAT',
           help='Scaling factor before the pre-conditioner.')
parser.add('--r-scale-learned', action='store_true',
           help='Learn scaling factor before the pre-conditioner '
                'as a learnable parameter')
parser.add('--r-norm', type=str, choices=['eye', 'tanh', 'sigmoid4', 'sigmoid3', 'sigmoid', 'sigmoid1', 'sigmoid05', 'exp', 'softplus', 'Gaussian', 'Gaussian1'],
           help='Normalization applied to the pre-conditioners before they are '
                'applied to the gradients.')

parser.add('--q-use', action='store_true',
           help='Use the pre-conditioners generated by LSTM.')
parser.add('--q-scale', type=float, default=1.0, metavar='FLOAT',
           help='Scaling factor before the pre-conditioner.')
parser.add('--q-scale-learned', action='store_true',
           help='Learn scaling factor before the pre-conditioner '
                'as a learnable parameter')
parser.add('--q-norm', type=str, choices=['eye', 'tanh', 'sigmoid4', 'sigmoid3', 'sigmoid', 'sigmoid1', 'sigmoid05', 'exp', 'softplus', 'Gaussian', 'Gaussian1'],
           help='Normalization applied to the pre-conditioners before they are '
                'applied to the gradients.')

parser.add('--h-use', action='store_true',
           help='Use the pre-conditioners generated by LSTM.')
parser.add('--h-scale', type=float, default=1.0, metavar='FLOAT',
           help='Scaling factor before the pre-conditioner.')
parser.add('--h-scale-learned', action='store_true',
           help='Learn scaling factor before the pre-conditioner '
                'as a learnable parameter')
parser.add('--h-norm', type=str, choices=['eye', 'tanh', 'sigmoid4', 'sigmoid3', 'sigmoid', 'sigmoid1', 'sigmoid05', 'exp', 'softplus', 'Gaussian', 'Gaussian1'],
           help='Normalization applied to the pre-conditioners before they are '
                'applied to the gradients.')

parser.add('--a-use', action='store_true',
           help='Use the bias terms generated by LSTM.')
parser.add('--a-scale', type=float, default=1.0, metavar='FLOAT',
           help='Scaling factor before the bias term')
parser.add('--a-scale-learned', action='store_true',
           help='Learn scaling factor before the bias term '
                'as a learnable parameter')
parser.add('--a-norm', type=str, choices=['eye', 'tanh', 'sigmoid4', 'sigmoid3', 'sigmoid', 'sigmoid1', 'sigmoid05', 'exp', 'softplus', 'tanh', 'Gaussian', 'Gaussian1'],
           help='Normalization applied to the bias terms before they are '
                'applied.')

parser.add('--b-use', action='store_true',
           help='Use the bias terms generated by LSTM.')
parser.add('--b-scale', type=float, default=1.0, metavar='FLOAT',
           help='Scaling factor before the bias term')
parser.add('--b-scale-learned', action='store_true',
           help='Learn scaling factor before the bias term '
                'as a learnable parameter')
parser.add('--b-norm', type=str, choices=['eye', 'tanh', 'sigmoid4', 'sigmoid3', 'sigmoid', 'sigmoid1', 'sigmoid05', 'exp', 'softplus', 'tanh', 'Gaussian', 'Gaussian1'],
           help='Normalization applied to the bias terms before they are '
                'applied.')

parser.add('--c-use', action='store_true',
           help='Use the pre-conditioners generated by LSTM.')
parser.add('--c-scale', type=float, default=1.0, metavar='FLOAT',
           help='Scaling factor before the pre-conditioner.')
parser.add('--c-scale-learned', action='store_true',
           help='Learn scaling factor before the pre-conditioner '
                'as a learnable parameter')
parser.add('--c-norm', type=str, choices=['eye', 'tanh', 'sigmoid4', 'sigmoid3', 'sigmoid', 'sigmoid1', 'sigmoid05', 'exp', 'softplus', 'Gaussian', 'Gaussian1'],
           help='Normalization applied to the pre-conditioners before they are '
                'applied to the gradients.')

parser.add('--d-use', action='store_true',
           help='Use the bias terms generated by LSTM.')
parser.add('--d-scale', type=float, default=1.0, metavar='FLOAT',
           help='Scaling factor before the bias term')
parser.add('--d-scale-learned', action='store_true',
           help='Learn scaling factor before the bias term '
                'as a learnable parameter')
parser.add('--d-norm', type=str, choices=['eye', 'tanh', 'sigmoid4', 'sigmoid3', 'sigmoid', 'sigmoid1', 'sigmoid05', 'exp', 'softplus', 'Gaussian', 'Gaussian1'],
           help='Normalization applied to the bias terms before they are '
                'applied.')

parser.add('--e-use', action='store_true',
           help='Use the bias terms generated by LSTM.')
parser.add('--e-scale', type=float, default=1.0, metavar='FLOAT',
           help='Scaling factor before the bias term')
parser.add('--e-scale-learned', action='store_true',
           help='Learn scaling factor before the bias term '
                'as a learnable parameter')
parser.add('--e-norm', type=str, choices=['eye', 'tanh', 'sigmoid4', 'sigmoid3', 'sigmoid', 'sigmoid1', 'sigmoid05', 'exp', 'softplus', 'Gaussian', 'Gaussian1'],
           help='Normalization applied to the bias terms before they are '
                'applied.')

parser.add('--f-use', action='store_true',
           help='Use the bias terms generated by LSTM.')
parser.add('--f-scale', type=float, default=1.0, metavar='FLOAT',
           help='Scaling factor before the bias term')
parser.add('--f-scale-learned', action='store_true',
           help='Learn scaling factor before the bias term '
                'as a learnable parameter')
parser.add('--f-norm', type=str, choices=['eye', 'tanh', 'sigmoid4', 'sigmoid3', 'sigmoid', 'sigmoid1', 'sigmoid05', 'exp', 'softplus', 'Gaussian', 'Gaussian1'],
           help='Normalization applied to the bias terms before they are '
                'applied.')

parser.add('--b3-use', action='store_true',
           help='Use the bias terms generated by LSTM.')
parser.add('--b3-scale', type=float, default=1e-2, metavar='FLOAT',
           help='Scaling factor before the bias term')
parser.add('--b3-scale-learned', action='store_true',
           help='Learn scaling factor before the bias term '
                'as a learnable parameter')
parser.add('--b3-norm', type=str, choices=['eye', 'tanh', 'sigmoid4', 'sigmoid3', 'sigmoid', 'sigmoid1', 'sigmoid05', 'exp', 'softplus', 'Gaussian', 'Gaussian1'],
           help='Normalization applied to the bias terms before they are '
                'applied.')

parser.add('--b5-use', action='store_true',
           help='Use the bias terms generated by LSTM.')
parser.add('--b5-scale', type=float, default=1e-2, metavar='FLOAT',
           help='Scaling factor before the bias term')
parser.add('--b5-scale-learned', action='store_true',
           help='Learn scaling factor before the bias term '
                'as a learnable parameter')
parser.add('--b5-norm', type=str, choices=['eye', 'tanh', 'sigmoid4', 'sigmoid3', 'sigmoid', 'sigmoid1', 'sigmoid05', 'exp', 'softplus', 'Gaussian', 'Gaussian1'],
           help='Normalization applied to the bias terms before they are '
                'applied.')

parser.add('--b4-use', action='store_true',
           help='Use the bias terms generated by LSTM.')
parser.add('--b4-scale', type=float, default=1e-2, metavar='FLOAT',
           help='Scaling factor before the bias term')
parser.add('--b4-scale-learned', action='store_true',
           help='Learn scaling factor before the bias term '
                'as a learnable parameter')
parser.add('--b4-norm', type=str, choices=['eye', 'tanh', 'sigmoid4', 'sigmoid3', 'sigmoid', 'sigmoid1', 'sigmoid05', 'exp', 'softplus', 'Gaussian', 'Gaussian1'],
           help='Normalization applied to the bias terms before they are '
                'applied.')

parser.add('--p-use', action='store_true',
           help='Use the pre-conditioners generated by LSTM.')
parser.add('--p-scale', type=float, default=1.0, metavar='FLOAT',
           help='Scaling factor before the pre-conditioner.')
parser.add('--p-scale-learned', action='store_true',
           help='Learn scaling factor before the pre-conditioner '
                'as a learnable parameter')
parser.add('--p-norm', type=str, choices=['eye', 'sigmoid', 'exp', 'softplus'],
           help='Normalization applied to the pre-conditioners before they are '
                'applied to the gradients.')

parser.add('--b1-use', action='store_true',
           help='Use the bias terms generated by LSTM.')
parser.add('--b1-scale', type=float, default=1e-2, metavar='FLOAT',
           help='Scaling factor before the bias term')
parser.add('--b1-scale-learned', action='store_true',
           help='Learn scaling factor before the bias term '
                'as a learnable parameter')
parser.add('--b1-norm', type=str, choices=['eye', 'sigmoid', 'exp', 'softplus'],
           help='Normalization applied to the bias terms before they are '
                'applied.')

parser.add('--b2-use', action='store_true',
           help='Use the bias terms generated by LSTM.')
parser.add('--b2-scale', type=float, default=1e-2, metavar='FLOAT',
           help='Scaling factor before the bias term')
parser.add('--b2-scale-learned', action='store_true',
           help='Learn scaling factor before the bias term '
                'as a learnable parameter')
parser.add('--b2-norm', type=str, choices=['eye', 'sigmoid', 'exp', 'softplus'],
           help='Normalization applied to the bias terms before they are '
                'applied.')

# Parameters of classic optimizers
parser.add('--step-size', type=float, default=None, metavar='FLOAT',
           help='Step size for the classic optimizers')
parser.add('--momentum1', type=float, default=None, metavar='FLOAT',
           help='decay factor of 1st order momentum (adam)')
parser.add('--momentum2', type=float, default=None, metavar='FLOAT',
           help='decay factor of 2nd order momentum (adam)')
parser.add('--eps', type=float, default=None, metavar='FLOAT',
           help='epsilon on adam optimizer')
parser.add('--hyper-step', type=float, default=None, metavar='FLOAT',
           help='Hyper step size of AdamHD')

# Parameter of B, C Shrinking
parser.add('--B-step-size', type=str, default=None, metavar='STR',
           help='Step size for Parameter B to alleviate OOD')
parser.add('--C-step-size', type=str, default=None, metavar='STR',
           help='Step size for Parameter C to alleviate OOD')

# Data parameters
parser.add('--seed', type=int, default=118, metavar='INT',
           help='Random seed for reproducibility')

# Training parameters
# parser.add('--objective', type=str, default='GT', metavar='{OBJECTIVE,L2,L1,GT}',
#            help='Objective used for the training')
parser.add('--save-dir', type=str, default='temp', metavar='STR',
           help='Saving directory for saved checkpoints and logs')
parser.add('--ckpt-path', type=str, default=None, metavar='STR',
           help='Path to the checkpoint to be loaded.')
parser.add('--loss-save-path', type=str, default=None, metavar='STR',
           help='Path to save the testing losses.')

# Training
parser.add('--global-training-steps', type=int, default=1000,
           help='Total number of training steps considered.')
parser.add('--optimizer-training-steps', type=int, default=100,
           help='Total number of batches of optimizees generated for training.')
parser.add('--unroll-length', type=int, default=1000,
           help='Total number of training steps considered.')

parser.add('--train-batch-size', type=int, default=128, metavar='N',
           help='Batch size for training')
parser.add('--val-batch-size', type=int, default=256, metavar='N',
           help='Batch size for validation')
parser.add('--test-batch-size', type=int, default=None, metavar='N',
           help='Batch size for testing')
parser.add('--val-size', type=int, default=2048, metavar='N',
           help='Number of validation samples')
parser.add('--test-size', type=int, default=2048, metavar='N',
           help='Number of testing samples')

parser.add('--print-freq', type=int, default=200,
           help='Frequency of printing training information')
parser.add('--val-freq', type=int, default=200,
           help='Frequency of validation')
parser.add('--val-length', type=int, default=100,
           help='Total length of optimization during validation')
parser.add('--test-length', type=int, default=100,
           help='Total length of optimization during testing')

parser.add('--init-lr', type=float, default=0.1, metavar='FLOAT',
           help='Initial learning rate')
parser.add('--scheduler', type=str, default='constant', metavar='STR',
           help='Learning rate scheduler.')
parser.add('--best-wait', type=int, default=5, metavar='N',
           help='Wait time for better validation performance')

parser.add('--epochs', type=int, default=5, help='Epochs')

parser.add('--clip-grad', action='store_true', help='if clip gradient')

parser.add('--debug', action='store_true', help='if use tensorboard')

parser.add('--loss-func', type=str, metavar='STR',
           help='What loss function to use.')


opts, _ = parser.parse_known_args()

# Save directory
opts.save_dir = os.path.join('results', opts.save_dir)
if not os.path.isdir(opts.save_dir):
    os.makedirs(opts.save_dir)
# Logging file
logger_file = os.path.join(opts.save_dir, 'train.log')
opts.logger = utils.setup_logger(logger_file)
opts.logger('Checkpoints will be saved to directory `{}`'.format(opts.save_dir))
opts.logger(
    'Log file for training will be saved to file `{}`'.format(logger_file))

# opts.x_norm_logger = utils.setup_logger(
#     os.path.join(opts.save_dir, 'x_norm.log'))

# opts.p_logger = utils.setup_logger(
#     os.path.join(opts.save_dir, 'p_norm_train.log'))
# opts.q_logger = utils.setup_logger(
#     os.path.join(opts.save_dir, 'q_norm_train.log'))
# opts.h_logger = utils.setup_logger(
#     os.path.join(opts.save_dir, 'h_norm_train.log'))
# opts.c_logger = utils.setup_logger(
#     os.path.join(opts.save_dir, 'c_norm_train.log'))

# opts.b_logger = utils.setup_logger(
#     os.path.join(opts.save_dir, 'b_norm_train.log'))

# opts.b3_logger = utils.setup_logger(
#     os.path.join(opts.save_dir, 'b3_norm_train.log'))
# opts.b5_logger = utils.setup_logger(
#     os.path.join(opts.save_dir, 'b5_norm_train.log'))
# opts.b4_logger = utils.setup_logger(
#     os.path.join(opts.save_dir, 'b4_norm_train.log'))

# Use cuda if it is available
if opts.cpu:
    opts.device = 'cpu'
elif opts.device is None:
    if torch.cuda.is_available():
        opts.device = 'cuda'
    else:
        opts.device = 'cpu'
        opts.logger('WARNING: No CUDA available. Run on CPU instead.')
# Output the type of device used
opts.logger('Using device: {}'.format(opts.device))
opts.dtype = torch.float
# opts.logger('Using tau: {}'.format(opts.tau)) # Output the tau used in current exp

# Set random seed for reproducibility
torch.manual_seed(opts.seed)
random.seed(opts.seed + 7)
np.random.seed(opts.seed + 42)

# -----------------------------------------------------------------------
#              Create data for training and validation
# -----------------------------------------------------------------------
# train_seen_loader, val_seen_loader, test_seen_loader, A, W, W_gram, G = create_sc_data(opts)
# A_TEN = torch.from_numpy(A).to(device=opts.device, dtype=opts.dtype)

if opts.fixed_dict:
    W = torch.randn(opts.input_dim, opts.output_dim).to(opts.device)
else:
    W = None

# Keyword artuments for the optimizers
optimizer_kwargs = {
    'r_use': opts.r_use,
    'r_scale': opts.r_scale,
    'r_scale_learned': opts.r_scale_learned,
    'r_norm': opts.r_norm,

    'q_use': opts.q_use,
    'q_scale': opts.q_scale,
    'q_scale_learned': opts.q_scale_learned,
    'q_norm': opts.q_norm,

    'h_use': opts.h_use,
    'h_scale': opts.h_scale,
    'h_scale_learned': opts.h_scale_learned,
    'h_norm': opts.h_norm,

    'a_use': opts.a_use,
    'a_scale': opts.a_scale,
    'a_scale_learned': opts.a_scale_learned,
    'a_norm': opts.a_norm,

    'b_use': opts.b_use,
    'b_scale': opts.b_scale,
    'b_scale_learned': opts.b_scale_learned,
    'b_norm': opts.b_norm,

    'c_use': opts.c_use,
    'c_scale': opts.c_scale,
    'c_scale_learned': opts.c_scale_learned,
    'c_norm': opts.c_norm,

    'd_use': opts.d_use,
    'd_scale': opts.d_scale,
    'd_scale_learned': opts.d_scale_learned,
    'd_norm': opts.d_norm,

    'e_use': opts.e_use,
    'e_scale': opts.e_scale,
    'e_scale_learned': opts.e_scale_learned,
    'e_norm': opts.e_norm,

    'f_use': opts.f_use,
    'f_scale': opts.f_scale,
    'f_scale_learned': opts.f_scale_learned,
    'f_norm': opts.f_norm,

    'b3_use': opts.b3_use,
    'b3_scale': opts.b3_scale,
    'b3_scale_learned': opts.b3_scale_learned,
    'b3_norm': opts.b3_norm,

    'b5_use': opts.b5_use,
    'b5_scale': opts.b5_scale,
    'b5_scale_learned': opts.b5_scale_learned,
    'b5_norm': opts.b5_norm,

    'b4_use': opts.b4_use,
    'b4_scale': opts.b4_scale,
    'b4_scale_learned': opts.b4_scale_learned,
    'b4_norm': opts.b4_norm,

    'p_use': opts.p_use,
    'p_scale': opts.p_scale,
    'p_scale_learned': opts.p_scale_learned,
    'p_norm': opts.p_norm,

    'b1_use': opts.b1_use,
    'b1_scale': opts.b1_scale,
    'b1_scale_learned': opts.b1_scale_learned,
    'b1_norm': opts.b1_norm,

    'b2_use': opts.b2_use,
    'b2_scale': opts.b2_scale,
    'b2_scale_learned': opts.b2_scale_learned,
    'b2_norm': opts.b2_norm,
}

reset_state_kwargs = {
    'state_scale': opts.state_scale,
    # 'step_size': opts.step_size,
    'momentum1': opts.momentum1,
    'momentum2': opts.momentum2,
    'eps': opts.eps,
    'hyper_step': opts.hyper_step,
    'B_step_size': opts.B_step_size,
    'C_step_size': opts.C_step_size,
}

# Keyword arguments for the optimizees
optimizee_kwargs = {
    'input_dim': opts.input_dim,
    'output_dim': opts.output_dim,
    'rho': opts.rho,
    's': opts.sparsity,
    'device': opts.device,
    'ood': opts.ood,
    'ood_s': opts.ood_s,
    'ood_t': opts.ood_t,
}

if opts.optimizer == 'ProximalGradientDescent':
    optimizer = optimizers.ProximalGradientDescent()
elif opts.optimizer == 'ProximalGradientDescentMomentum':
    optimizer = optimizers.ProximalGradientDescentMomentum()
elif opts.optimizer == 'SubGradientDescent':
    optimizer = optimizers.SubGradientDescent()
elif opts.optimizer == 'Adam':
    optimizer = optimizers.Adam()
elif opts.optimizer == 'AdamHD':
    optimizer = optimizers.AdamHD()
elif opts.optimizer == 'Shampoo':
    optimizer = optimizers.Shampoo()
elif opts.optimizer == 'CoordMathLSTM':
    optimizer = optimizers.CoordMathLSTM(
        input_size=2,
        output_size=1,
        hidden_size=opts.lstm_hidden_size,
        layers=opts.lstm_layers,
        **optimizer_kwargs
    )
elif opts.optimizer == 'RNNprop':
    optimizer = optimizers.RNNprop(
        input_size=2,
        output_size=1,
        hidden_size=opts.lstm_hidden_size,
        layers=opts.lstm_layers,
        beta1=opts.rnnprop_beta1,
        beta2=opts.rnnprop_beta2,
        **optimizer_kwargs
    )
elif opts.optimizer == 'CoordBlackboxLSTM':
    optimizer = optimizers.CoordBlackboxLSTM(
        input_size=2,
        output_size=1,
        hidden_size=opts.lstm_hidden_size,
        layers=opts.lstm_layers,
        **optimizer_kwargs
    )
elif opts.optimizer == 'GOMathL2O':
    optimizer = optimizers.GOMathL2O(
        input_size=3,
        output_size=1,
        hidden_size=opts.lstm_hidden_size,
        layers=opts.lstm_layers,
        **optimizer_kwargs
    )
elif opts.optimizer == 'GOMathL2OSTD':
    optimizer = optimizers.GOMathL2OSTD(
        input_size=3,
        output_size=1,
        hidden_size=opts.lstm_hidden_size,
        layers=opts.lstm_layers,
        **optimizer_kwargs
    )
elif opts.optimizer == 'GOMathL2OLH':
    optimizer = optimizers.GOMathL2OLH(
        input_size=3,
        output_size=1,
        hidden_size=opts.lstm_hidden_size,
        layers=opts.lstm_layers,
        **optimizer_kwargs
    )
else:
    raise ValueError(f'Invalid optimizer name {opts.optimizer}')


def unroll_BP(unroll_length, optimizer_training_steps, meta_optimizer, optimizer, optimizees, opts, verbose):
    is_nan = False
    training_losses = []
    num_roll_segs = optimizer_training_steps // unroll_length
    for num_roll in range(num_roll_segs):
        global_loss = 0.0
        start = timer()

        # loss_last = None
        all_weights = sum(range(1, unroll_length + 1))
        for j in range(unroll_length):
            optimizees = optimizer(optimizees, opts.grad_method)
            # opts.logger("global step {} num roll {} unroll length {} X norm {}".format(
            #     i, num_roll, j, torch.mean(optimizees.X)))
            loss = optimizees.objective(compute_grad=True)
            if torch.isinf(loss) or torch.isnan(loss):
                is_nan = True
                break
            if opts.loss_func == 'mean':
                global_loss += loss / unroll_length
            elif opts.loss_func == 'weighted_sum':
                global_loss += loss * ((j+1) / all_weights)
            else:
                raise ValueError(
                    f'Invalid loss function name {opts.loss_func}')
            # opts.logger('{} {} {}'.format(num_roll, j, loss))
        if is_nan:
            break
        else:
            meta_optimizer.zero_grad()
            global_loss.backward()  # retain_graph=True
            if opts.clip_grad:
                torch.nn.utils.clip_grad_norm_(
                    optimizer.parameters(), 1.0, error_if_nonfinite=False)
            # print(i, num_roll, norm)
            meta_optimizer.step()
            training_losses.append(global_loss.detach().cpu().item())

            # Clean up the current unrolling segments, including:
            # - Detach the current hidden and cell states.
            # - Clear the `hist` list of the optimizers to release memory
            optimizer.detach_state()
            optimizees.detach_vars()

            # optimizer.clear_hist()

            time = timer() - start
            if verbose:
                opts.logger(
                    '--> time consuming [{:.4f}s] optimizer train steps :  [{}] '
                    '| Global_Loss = [{:.4f}]'.format(
                        time,
                        (num_roll + 1) * unroll_length,
                        training_losses[-1]
                    )
                )
    return is_nan, optimizer, meta_optimizer


if not opts.test:
    torch.autograd.set_detect_anomaly(True)
    config_path = os.path.join(opts.save_dir, 'config.yaml')
    parser.write_config_file(opts, [config_path])

    assert isinstance(
        optimizer, nn.Module), 'Only PyTorch Modules need training.'

    optimizer = optimizer.to(device=opts.device, dtype=opts.dtype)
    # , weight_decay=1e-3
    meta_optimizer = optim.Adam(
        optimizer.parameters(), lr=opts.init_lr)
    if opts.scheduler == 'cosine':
        meta_scheduler = optim.lr_scheduler.CosineAnnealingLR(
            meta_optimizer, T_max=opts.global_training_steps, eta_min=1e-5)
    elif opts.scheduler == 'constant':
        meta_scheduler = optim.lr_scheduler.ConstantLR(
            meta_optimizer, factor=1.0, total_iters=opts.global_training_steps)
    elif opts.scheduler == 'step':
        meta_scheduler = optim.lr_scheduler.StepLR(
            meta_optimizer, step_size=1, gamma=0.1)
    else:
        raise NotImplementedError

    training_losses = []  # initialize the array storing training loss function
    best_validation_mean = 99999999999999
    best_validation_final = 99999999999999

    if opts.debug:
        writer = SummaryWriter(opts.save_dir)
    eval_step = 0
    for epoch in range(opts.epochs):
        # if e >= 2:
        #     opts.optimizer_training_steps = 30
        #     opts.unroll_length = 30

        for i in range(opts.global_training_steps):
            seed = opts.seed + 77 * (i+1)
            if (i+1) % opts.print_freq == 0:
                verbose = True
                opts.logger('\n=============> Epoch: {} LR: {}'.format(
                    epoch, meta_scheduler.get_last_lr()))
                opts.logger(
                    '\n=============> global training steps: {}'.format(i))
            else:
                verbose = False
            optimizer.train()

            optimizees = OPTIMIZEE_DICT[opts.optimizee_type](
                opts.train_batch_size, W, seed=seed, **optimizee_kwargs
            )
            # writer.add_scalar('train/L_smoothness_mean',
            #                   optimizees.grad_lipschitz().mean(), i)
            optimizer.reset_state(
                optimizees, opts.step_size, **reset_state_kwargs)
            is_nan, optimizer, meta_optimizer = unroll_BP(opts.unroll_length, opts.optimizer_training_steps,
                                                          meta_optimizer, optimizer, optimizees, opts, verbose)
            if is_nan:
                opts.logger(
                    '\n=============> Objective NaN 1st Time, try {}-{}'.format(5, opts.optimizer_training_steps))
                is_nan, optimizer, meta_optimizer = unroll_BP(5, opts.optimizer_training_steps,
                                                              meta_optimizer, optimizer, optimizees, opts, verbose)

            if is_nan:
                opts.logger(
                    '\n=============> Objective NaN 2nd Time, try {}-{}'.format(3, 60))
                is_nan, optimizer, meta_optimizer = unroll_BP(3, 60,
                                                              meta_optimizer, optimizer, optimizees, opts, verbose)

            if is_nan:
                opts.logger(
                    '\n=============> Objective NaN 3nd Time, try {}-{}'.format(1, 20))
                is_nan, optimizer, meta_optimizer = unroll_BP(1, 20,
                                                              meta_optimizer, optimizer, optimizees, opts, verbose)

            if is_nan:
                opts.logger(
                    '\n=============> Objective NaN 3nd Time, try {}-{}'.format(1, 5))
                is_nan, optimizer, meta_optimizer = unroll_BP(1, 5,
                                                              meta_optimizer, optimizer, optimizees, opts, verbose)

            if is_nan:
                opts.logger('\n=============> Sample {} All NaN !!!'.format(i))

            if (i+1) % opts.val_freq == 0:
                optimizer.eval()
                optimizees = OPTIMIZEE_DICT[opts.optimizee_type](
                    opts.val_size, W, seed=118 + 77, **optimizee_kwargs
                )
                optimizer.reset_state(
                    optimizees, opts.step_size, **reset_state_kwargs)
                validation_losses = []
                validation_grad = []
                validation_grad_dirc = []
                if opts.debug:
                    if opts.optimizer == 'GOMathL2O':
                        R, Q, H, A, B, C, = [], [], [], [], [], []
                        b1, b2, b3 = [], [], []
                    elif opts.optimizer == 'CoordMathLSTM':
                        P, A2 = [], []
                for j in range(opts.val_length):
                    # Fixed data samples for validation
                    optimizees = optimizer(optimizees, opts.grad_method)
                    # # L1-norm + l1_lambda * l1_norm
                    # l1_lambda = 0.1
                    # l1_norm = sum(p.abs().sum() for p in optimizer.parameters())
                    loss = optimizees.objective()
                    validation_losses.append(loss.detach().cpu().item())
                    grad = optimizees.get_grad(
                        grad_method='subgrad',
                        compute_grad=False,
                        retain_graph=False
                    )
                    validation_grad.append(torch.linalg.norm(
                        grad, dim=(-2, -1), ord='fro', keepdim=False).mean().detach().cpu().item())
                    validation_grad_dirc.append(torch.sign(
                        grad).sum((-2, -1)).mean().detach().cpu().item())
                    if opts.debug:
                        if opts.optimizer == 'GOMathL2O':
                            if opts.r_use:
                                R.append(
                                    optimizer.R.mean().detach().cpu().item())
                            if opts.q_use:
                                Q.append(
                                    optimizer.Q.mean().detach().cpu().item())
                            if opts.h_use:
                                H.append(
                                    optimizer.H.mean().detach().cpu().item())
                            if opts.a_use:
                                A.append(
                                    optimizer.A.mean().detach().cpu().item())
                            if opts.b_use:
                                B.append(
                                    optimizer.B.mean().detach().cpu().item())
                            if opts.c_use:
                                C.append(
                                    optimizer.C.mean().detach().cpu().item())
                            if opts.b3_use:
                                b1.append(
                                    torch.linalg.norm(
                                        optimizer.b1, dim=(-2, -1), ord='fro', keepdim=False
                                    ).mean().detach().cpu().item())
                            if opts.b4_use:
                                b2.append(
                                    torch.linalg.norm(
                                        optimizer.b2, dim=(-2, -1), ord='fro', keepdim=False
                                    ).mean().detach().cpu().item())
                            if opts.b5_use:
                                b3.append(
                                    torch.linalg.norm(
                                        optimizer.b3, dim=(-2, -1), ord='fro', keepdim=False
                                    ).mean().detach().cpu().item())
                        elif opts.optimizer == 'CoordMathLSTM':
                            P.append(optimizer.P.mean().detach().cpu().item())
                            A2.append(optimizer.A.mean().detach().cpu().item())
                if opts.debug:
                    for e in range(0, opts.val_length, 10):
                        writer.add_scalar(
                            "eval/Obj-Step"+str(e+1), validation_losses[e], eval_step)
                        writer.add_scalar(
                            "eval/Grad-Step"+str(e+1), validation_grad[e], eval_step)
                        writer.add_scalar(
                            "eval/Grad-Target-Step"+str(e+1), validation_grad_dirc[e], eval_step)
                        if opts.optimizer == 'GOMathL2O':
                            if opts.r_use:
                                writer.add_scalar(
                                    "R/Step"+str(e+1), R[e], eval_step)
                            if opts.q_use:
                                writer.add_scalar(
                                    "Q/Step"+str(e+1), Q[e], eval_step)
                            if opts.h_use:
                                writer.add_scalar(
                                    "H/Step"+str(e+1), H[e], eval_step)
                            if opts.a_use:
                                writer.add_scalar(
                                    "A/Step"+str(e+1), A[e], eval_step)
                            if opts.b_use:
                                writer.add_scalar(
                                    "B/Step"+str(e+1), B[e], eval_step)
                            if opts.c_use:
                                writer.add_scalar(
                                    "C/Step"+str(e+1), C[e], eval_step)
                            if opts.b3_use:
                                writer.add_scalar(
                                    "b1/Step"+str(e+1), b1[e], eval_step)
                            if opts.b4_use:
                                writer.add_scalar(
                                    "b2/Step"+str(e+1), b2[e], eval_step)
                            if opts.b5_use:
                                writer.add_scalar(
                                    "b3/Step"+str(e+1), b3[e], eval_step)
                        elif opts.optimizer == 'CoordMathLSTM':
                            writer.add_scalar(
                                "P/Step"+str(e+1), P[e], eval_step)
                            writer.add_scalar(
                                "A2/Step"+str(e+1), A2[e], eval_step)

                    eval_step += 1
                # if (validation_losses[-1] < best_validation_final and
                #         np.mean(validation_losses) < best_validation_mean) :
                if np.mean(validation_losses) < best_validation_mean:
                    best_validation_final = validation_losses[-1]
                    best_validation_mean = np.mean(validation_losses)
                    opts.logger(
                        '\n\n===> best of final LOSS[{}]:={} LOSS[{}]:={} LOSS[{}]:={}, '
                        'best_mean_loss ={}'.format(
                            30, validation_losses[29],
                            50, validation_losses[49],
                            100, validation_losses[99],
                            best_validation_mean
                        )
                    )

                    checkpoint_name = optimizer.name() + '.pth'
                    save_path = os.path.join(opts.save_dir, checkpoint_name)
                    torch.save(optimizer.state_dict(), save_path)
                    opts.logger('Saved the optimizer to file: ' + save_path)
        meta_scheduler.step()
    if opts.debug:
        writer.close()
else:
    opts.logger(f'********* OOD state {str(opts.ood)} *********')
    if isinstance(optimizer, nn.Module):
        checkpoint_name = optimizer.name() + '.pth'
        if not opts.ckpt_path:
            opts.ckpt_path = os.path.join(opts.save_dir, checkpoint_name)
        optimizer.load_state_dict(torch.load(
            opts.ckpt_path, map_location='cpu'))
        opts.logger(f'Trained weight loaded from {opts.ckpt_path}')
        optimizer.to(device=opts.device, dtype=opts.dtype).eval()
        optimizer.eval()

    if not opts.test_batch_size:
        opts.test_batch_size = opts.test_size

    num_test_batches = opts.test_size // opts.test_batch_size
    test_losses = [0.0] * (opts.test_length + 1)
    if opts.save_sol:
        test_losses_batch = np.zeros(
            (opts.test_length + 1, opts.test_batch_size))

    time_start = time.time()
    time_opt = 0

    if opts.debug:
        if opts.optimizer == 'GOMathL2O':
            R, Q, H, A, B, C, = [0.0] * (opts.test_length), [0.0] * (opts.test_length), [0.0] * (
                opts.test_length), [0.0] * (opts.test_length), [0.0] * (opts.test_length), [0.0] * (opts.test_length)
            # Input of LSTM
            lstm_input, lstm_input_c, lstm_input_d = [
                0.0] * (opts.test_length), [0.0] * (opts.test_length), [0.0] * (opts.test_length)
        elif opts.optimizer == 'CoordMathLSTM':
            P, A2 = [0.0] * (opts.test_length), [0.0] * (opts.test_length)
    for i in range(num_test_batches):
        seed = opts.seed + 777 * (i+1)

        optimizees = OPTIMIZEE_DICT[opts.optimizee_type](
            opts.test_batch_size, None, seed=seed, **optimizee_kwargs
        )

        if opts.load_mat:
            optimizees.load_from_file(
                opts.optimizee_dir + '/' + str(i) + '.mat', 0, opts.test_batch_size)
            opts.logger('Batch {} optimizee loaded.'.format(i))

        if opts.load_sol:
            optimizees.load_sol(opts.optimizee_dir + '/sol_' +
                                str(i) + '.mat', 0, opts.test_batch_size)
            opts.logger('Batch {} optimal objective loaded.'.format(i))

        if opts.save_to_mat:
            if not os.path.exists(opts.optimizee_dir):
                os.makedirs(opts.optimizee_dir, exist_ok=True)
            optimizees.save_to_file(opts.optimizee_dir + '/' + str(i) + '.mat')

        optimizer.reset_state(optimizees, opts.step_size, **reset_state_kwargs)
        if not opts.load_sol:
            test_losses[0] += optimizees.objective().detach().cpu().item()
        else:
            test_losses[0] += optimizees.objective_shift().detach().cpu().item()

        if opts.save_sol:
            test_losses_batch[0] = optimizees.objective_batch().cpu().numpy()

        for j in range(opts.test_length):

            time_inner_start = time.time()
            # print(j)
            optimizees = optimizer(optimizees, opts.grad_method)
            optimizer.detach_state()
            time_inner_end = time.time()
            time_opt += (time_inner_end - time_inner_start)

            if not opts.load_sol:
                loss = optimizees.objective()
            else:
                loss = optimizees.objective_shift()

            test_losses[j+1] += loss.detach().cpu().item()
            if opts.save_sol:
                test_losses_batch[j +
                                  1] = optimizees.objective_batch().cpu().numpy()

            if opts.debug:
                if opts.optimizer == 'GOMathL2O':
                    if opts.r_use:
                        R[j] += optimizer.R.mean().detach().cpu().item()
                    if opts.q_use:
                        Q[j] += optimizer.Q.mean().detach().cpu().item()
                    if opts.h_use:
                        H[j] += optimizer.H.mean().detach().cpu().item()
                    if opts.a_use:
                        A[j] += optimizer.A.mean().detach().cpu().item()
                    if opts.b_use:
                        B[j] += optimizer.B.mean().detach().cpu().item()
                    if opts.c_use:
                        C[j] += optimizer.C.mean().detach().cpu().item()
                    lstm_input[j] += torch.linalg.norm(
                        optimizer.lstm_input, dim=(-2, -1), ord='fro', keepdim=False).mean().detach().cpu().item()
                    lstm_input_c[j] += torch.linalg.norm(
                        optimizer.lstm_input_c, dim=(-2, -1), ord='fro', keepdim=False).mean().detach().cpu().item()
                    lstm_input_d[j] += torch.linalg.norm(
                        optimizer.lstm_input_d, dim=(-2, -1), ord='fro', keepdim=False).mean().detach().cpu().item()
                elif opts.optimizer == 'CoordMathLSTM':
                    P[j] += optimizer.P.mean().detach().cpu().item()
                    A2[j] += optimizer.A.mean().detach().cpu().item()
            # if j == 200:
            #     exit(0)
        opts.logger('Batch {} completed.'.format(i))

        if opts.save_sol:
            if not os.path.exists(opts.optimizee_dir):
                os.makedirs(opts.optimizee_dir, exist_ok=True)
            obj_star = np.min(test_losses_batch, axis=0)
            optimizees.save_sol(
                obj_star, opts.optimizee_dir + '/sol_' + str(i) + '.mat')
            opts.logger('Batch {} optimal objective saved.'.format(i))

        
    time_end = time.time()
    test_losses = [loss / num_test_batches for loss in test_losses]

    if opts.debug:
        if opts.optimizer == 'GOMathL2O':
            if opts.r_use:
                R = [r / num_test_batches for r in R]
                np.savetxt(os.path.join(opts.save_dir, 'R_S' +
                                        str(opts.ood_s) + 'T'+str(opts.ood_t)), np.array(R))
            if opts.q_use:
                Q = [q / num_test_batches for q in Q]
                np.savetxt(os.path.join(opts.save_dir, 'Q_S' +
                                        str(opts.ood_s) + 'T'+str(opts.ood_t)), np.array(Q))
            if opts.h_use:
                H = [h / num_test_batches for h in H]
                np.savetxt(os.path.join(opts.save_dir, 'H_S' +
                                        str(opts.ood_s) + 'T'+str(opts.ood_t)), np.array(H))
            if opts.a_use:
                A = [a / num_test_batches for a in A]
                np.savetxt(os.path.join(opts.save_dir, 'A_S' +
                                        str(opts.ood_s) + 'T'+str(opts.ood_t)), np.array(A))
            if opts.b_use:
                B = [b / num_test_batches for b in B]
                np.savetxt(os.path.join(opts.save_dir, 'B_S' +
                                        str(opts.ood_s) + 'T'+str(opts.ood_t)), np.array(B))
            if opts.c_use:
                C = [c / num_test_batches for c in C]
                np.savetxt(os.path.join(opts.save_dir, 'C_S' +
                                        str(opts.ood_s) + 'T'+str(opts.ood_t)), np.array(C))
            lstm_input = [l1 / num_test_batches for l1 in lstm_input]
            np.savetxt(os.path.join(opts.save_dir, 'lstm_input_S' +
                                    str(opts.ood_s) + 'T'+str(opts.ood_t)), np.array(lstm_input))
            lstm_input_c = [l2 / num_test_batches for l2 in lstm_input_c]
            np.savetxt(os.path.join(opts.save_dir, 'lstm_input_c_S' +
                                    str(opts.ood_s) + 'T'+str(opts.ood_t)), np.array(lstm_input_c))
            lstm_input_d = [l3 / num_test_batches for l3 in lstm_input_d]
            np.savetxt(os.path.join(opts.save_dir, 'lstm_input_d_S' +
                                    str(opts.ood_s) + 'T'+str(opts.ood_t)), np.array(lstm_input_d))
        elif opts.optimizer == 'CoordMathLSTM':
            P = [p / num_test_batches for p in P]
            np.savetxt(os.path.join(opts.save_dir, 'P_S' +
                                    str(opts.ood_s) + 'T'+str(opts.ood_t)), np.array(P))
            A2 = [a2 / num_test_batches for a2 in A2]
            np.savetxt(os.path.join(opts.save_dir, 'A_S' +
                                    str(opts.ood_s) + 'T'+str(opts.ood_t)), np.array(A2))

    # output the epoch results to the terminal
    opts.logger('Testing losses:')
    for ii, t_loss in enumerate(test_losses):
        opts.logger('{}, {}'.format(ii, t_loss))
    if not opts.loss_save_path:
        opts.loss_save_path = os.path.join(opts.save_dir, 'test_losses.txt')
    else:
        opts.loss_save_path = os.path.join(opts.save_dir, opts.loss_save_path)
    opts.logger(f'testing losses saved to {opts.loss_save_path}')
    np.savetxt(opts.loss_save_path, np.array(test_losses))

    opts.logger("Total time: {}".format(time_end - time_start))
    opts.logger("Time (opt iteration): {}".format(time_opt))
    opts.logger("Time per iter per instance: {}".format(
        time_opt / opts.test_length / opts.test_size))
